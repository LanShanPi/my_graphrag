# 理科教材知识图谱构建与检索：技术解析

## 简介
这套系统的目标是把理科教材里的知识点提取出来，构建一个可以查询的知识图谱。从文档解析开始，到最终检索出答案，整个流程用了很多技术，下面按顺序拆解一下，顺便贴上相关代码片段和参数设置，方便理解。

---

## 1. 文档解析与预处理

### 用到的技术
- **`python-docx`**：处理 Word 文档。
- **`PyMuPDF (fitz)`**：处理 PDF 文档，支持嵌入式文字和图片提取。
- **`PaddleOCR`**：OCR 技术，用于识别 PDF 图片中的文字。
- **正则表达式（`re`）**：匹配章节标题、公式等特定模式。

### 参数设置与代码示例
```python
# PaddleOCR 的参数配置
ocr = PaddleOCR(use_angle_cls=True, lang="ch")  # 启用文字方向分类，语言设置为中文

# 从 DOCX 文件读取内容
def read_docx(file_path):
    docx = DocxDocument(file_path)
    content = []
    for paragraph in docx.paragraphs:
        text = paragraph.text.strip()
        if text:
            content.append(text)
    for table in docx.tables:
        for row in table.rows:
            row_text = [cell.text.strip() for cell in row.cells if cell.text.strip()]
            if row_text:
                content.append(" | ".join(row_text))
    return content

# 从 PDF 提取文字，如果是图片就用 PaddleOCR
pdf = fitz.open(file_path)
for page in pdf:
    text = page.get_text("text")
    if not text.strip():
        text = extract_text_with_paddleocr(page)
```

### 作用
- 把教材内容提取成纯文本，无论是 PDF 里的文字还是图片里的文字，都能处理。
- **参数作用**：PaddleOCR 中 `use_angle_cls` 参数提高对文字方向不同的内容识别准确性，`lang` 参数确保处理中文内容。

---

## 2. 公式保护与分块

### 用到的技术
- **`SymPy`**：验证和解析数学公式。
- **正则表达式**：识别公式并用占位符替换。
- **`HanLP`**：用于分句和语义分块。

### 参数设置与代码示例
```python
# 使用正则表达式保护公式
def protect_formulas_advanced(text):
    formula_pattern = r"[a-zA-Z0-9_+\-*/^=<>≤≥(),\[\]{}|]+"
    formulas = re.findall(formula_pattern, text)
    formula_map = {}
    for i, formula in enumerate(formulas):
        placeholder = f"{{FORMULA_{i}}}"
        formula_map[placeholder] = formula
        text = text.replace(formula, placeholder)
    return text, formula_map

# 分块时结合章节信息
chunk_size = 1024  # 每个分块的大小
overlap = 256  # 分块之间的重叠部分
section_pattern = r"(第[一二三四五六七八九十百]+[章节]|[0-9]+\.[0-9]+.*?)"
matches = re.finditer(section_pattern, text)
```

### 作用
- **公式保护**：用占位符替换公式，防止公式在分块中被截断。
- **分块参数**：
  - `chunk_size` 控制每个分块的大小，设置为 1024 以平衡语义完整性和计算效率。
  - `overlap` 控制分块的重叠范围，避免分块之间的语义断层。

---

## 3. 知识图谱构建

### 用到的技术
- **`LlamaIndex`**：核心工具，用于抽取知识点三元组。
- **`OpenAI API`**：调用 GPT 模型进行三元组提取。
- **自定义 Prompt**：定义规则，限定实体和关系类型。

### 参数设置与代码示例
```python
# 自定义三元组抽取模板
CUSTOM_KG_TRIPLET_EXTRACT_PROMPT = PromptTemplate(
    triplet_extraction_template["CUSTOM_KG_TRIPLET_EXTRACT_TMPL"],
    prompt_type=PromptType.KNOWLEDGE_TRIPLET_EXTRACT
)

# 构建知识图谱
index = PropertyGraphIndex.from_documents(
    chunked_documents,
    llm=llm,
    embed_model=embed_model,
    include_embeddings=True,
    max_triplets_per_chunk=10,  # 每个分块最多提取 10 个三元组
    storage_context=storage_context,
    kg_triple_extract_template=CUSTOM_KG_TRIPLET_EXTRACT_PROMPT,
    allowed_entity_types=entity_types,  # 限定实体类型
    allowed_relation_types=relation_types,  # 限定关系类型
)
```

### 作用
- **自动提取知识点关系**：从文档中提取实体和关系，形成结构化的三元组。
- **参数设置**：
  - `max_triplets_per_chunk` 限制每个分块提取的三元组数量，避免信息冗余。
  - `allowed_entity_types` 和 `allowed_relation_types` 定义提取范围，确保内容相关性。

---

## 4. 数据存储与加载

### 用到的技术
- **`SimpleGraphStore`**：管理图数据。
- **`StorageContext`**：管理存储路径，支持断点续存。

### 参数设置与代码示例
```python
# 检查存储目录是否存在
def check_subfolder_exists(parent_folder, subfolder_name):
    subfolder_path = os.path.join(parent_folder, subfolder_name)
    return os.path.exists(subfolder_path) and os.path.isdir(subfolder_path)

# 存储上下文
storage_context = StorageContext.from_defaults(graph_store=graph_store)
storage_context.persist(persist_dir=index_dir)
```

### 作用
- **持久化存储**：保存知识图谱，避免重复构建。
- **断点续存**：根据存储路径判断是否需要重新生成。

---

## 5. 查询与回答

### 用到的技术
- **`LlamaIndex` 查询引擎**：支持多种查询模式（`tree_summarize`、`refine`、`compact`）。
- **OpenAI GPT**：利用大模型生成清晰、准确的回答。

### 参数设置与代码示例
```python
query_engine = index.as_query_engine(
    llm=llm,
    include_text=True,
    response_mode="tree_summarize",  # 使用递归总结模式
    similarity_top_k=20,  # 检索相似文档的数量
    vector_store_query_mode="SEMANTIC_HYBRID"  # 混合语义检索模式
)
response = query_engine.query(queries)
```

### 作用
- **语义检索**：基于三元组和向量搜索的语义匹配，返回符合用户问题的答案。
- **参数设置**：
  - `response_mode` 决定生成回答的逻辑方式。
  - `similarity_top_k` 限定参与检索的文档数量，提高查询效率。

---

## 6. 知识图谱可视化

### 用到的技术
- **`NetworkX`**：构建有向图结构。
- **`Pyvis`**：生成交互式 HTML 文件。

### 参数设置与代码示例
```python
# 可视化参数
net = Network(notebook=True, cdn_resources="in_line", directed=True)
net.from_nx(G)
html_file = os.path.join(storage_dir, f"{dir_name}_graph.html")
net.show(html_file)
```

### 作用
- **图谱展示**：将知识点和关系以节点和边的形式直观呈现。
- **参数作用**：`directed=True` 确保图谱有向，表示关系方向。

---

## 7. 日志记录

### 用到的技术
- **`logging`**：记录系统运行状态。

### 参数设置与代码示例
```python
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler("knowledge_graph.log"),
        logging.StreamHandler()
    ]
)
```

### 作用
- **操作记录**：记录知识图谱构建和查询过程，方便调试和优化。

---

## 总结
从文档解析到检索响应，系统整合了多种技术，详细参数配置确保功能可用性和效率：
1. **文档解析**：支持 PDF 和 DOCX 提取，OCR 处理图片文字。
2. **公式保护与分块**：保证公式和语义完整性，控制分块大小和重叠。
3. **知识图谱构建**：利用 GPT 提取三元组，支持自定义规则。
4. **存储与加载**：持久化存储，支持断点续存。
5. **查询与回答**：结合向量搜索和大模型生成答案，参数细化检索结果。
6. **可视化**：通过 HTML 文件展示知识图谱。
7. **日志记录**：跟踪运行状态，方便调试。

如果有其他问题或需要优化的地方，可以进一步讨论！

